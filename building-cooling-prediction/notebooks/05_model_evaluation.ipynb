{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Evaluate models using NRMSE with reproducible splits and generate plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "if 'notebooks' in os.getcwd():\n",
    "    sys.path.append('..')\n",
    "else:\n",
    "    sys.path.append('.')\n",
    "\n",
    "# Import custom modules\n",
    "try:\n",
    "    from src.evaluation.metrics import nrmse\n",
    "    from src.evaluation.validation import simple_train_test_split, k_fold_split, time_series_split\n",
    "    from src.evaluation.visualization import plot_predictions, plot_residuals\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "    print(\"Make sure you've run the previous notebooks to create the required modules.\")\n",
    "    raise\n",
    "\n",
    "# Check data availability\n",
    "data_path = '../data/processed/features_train.csv'\n",
    "if not os.path.exists(data_path):\n",
    "    print(f'Missing {data_path}. Run feature engineering first.')\n",
    "    # Create sample data for testing\n",
    "    print(\"Creating sample data for testing...\")\n",
    "    os.makedirs('../data/processed', exist_ok=True)\n",
    "    sample_data = pd.DataFrame({\n",
    "        'Total_Cooling_Load': np.random.normal(1000, 200, 1000),\n",
    "        'feature_1': np.random.normal(0, 1, 1000),\n",
    "        'feature_2': np.random.normal(0, 1, 1000)\n",
    "    })\n",
    "    sample_data.to_csv(data_path, index=False)\n",
    "    print(f\"Sample data created at {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (8760, 45)\n",
      "Columns: ['key_0', 'record_timestamp', 'CHR-01-KW', 'CHR-01-CHWSWT', 'CHR-01-CHWRWT', 'CHR-01-CHWFWR', 'CHR-02-KW', 'CHR-02-CHWSWT', 'CHR-02-CHWRWT', 'CHR-02-CHWFWR', 'CHR-03-KW', 'CHR-03-CHWSWT', 'CHR-03-CHWRWT', 'CHR-03-CHWFWR', 'CHR-01-CL', 'CHR-02-CL', 'CHR-03-CL', 'Total_Cooling_Load', 'hour', 'dayofweek', 'dayofyear', 'month', 'year', 'weekofyear', 'date', 'temperature_celsius', 'humidity_percent', 'wind_speed_kmh', 'CHR-01-delta_t', 'CHR-02-delta_t', 'CHR-03-delta_t', 'Total_Cooling_Load_lag_1', 'Total_Cooling_Load_rolling_mean_1', 'Total_Cooling_Load_lag_3', 'Total_Cooling_Load_rolling_mean_3', 'Total_Cooling_Load_lag_7', 'Total_Cooling_Load_rolling_mean_7', 'temperature_celsius_lag_1', 'temperature_celsius_rolling_mean_1', 'temperature_celsius_lag_3', 'temperature_celsius_rolling_mean_3', 'temperature_celsius_lag_7', 'temperature_celsius_rolling_mean_7', 'temp_x_hour', 'humidity_x_hour']\n",
      "Features shape: (8760, 41)\n",
      "Target shape: (8760,)\n",
      "Target range: [nan, nan]\n",
      "Train shape: (7008, 41), Test shape: (1752, 41)\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Target column\n",
    "target_col = 'Total_Cooling_Load'\n",
    "if target_col not in df.columns:\n",
    "    print(f\"Target column '{target_col}' not found. Available columns: {list(df.columns)}\")\n",
    "    # Use first numeric column as target for testing\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        target_col = numeric_cols[0]\n",
    "        print(f\"Using '{target_col}' as target column for testing.\")\n",
    "    else:\n",
    "        raise ValueError(\"No numeric columns found in data\")\n",
    "\n",
    "# Prepare features and target\n",
    "num_df = df.select_dtypes(include=[np.number])\n",
    "X = num_df.drop(columns=[target_col]).fillna(num_df.median(numeric_only=True))\n",
    "y = num_df[target_col].values\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target range: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "\n",
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = simple_train_test_split(\n",
    "    X.values, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Models (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded xgboost model from ../models/trained_models/xgboost_model.pkl\n",
      "Loaded lightgbm model from ../models/trained_models/lightgbm_model.pkl\n",
      "Loaded LSTM model from ../models/trained_models/lstm_model.h5\n",
      "\n",
      "Loaded models: ['lightgbm', 'lstm', 'xgboost']\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "model_paths = {\n",
    "    'xgboost': '../models/trained_models/xgboost_model.pkl',\n",
    "    'lightgbm': '../models/trained_models/lightgbm_model.pkl'\n",
    "}\n",
    "\n",
    "# Load tree-based models\n",
    "for name, path in model_paths.items():\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            models[name] = joblib.load(path)\n",
    "            print(f\"Loaded {name} model from {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {name} model: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file not found: {path}\")\n",
    "\n",
    "# Load LSTM model (multiple potential paths)\n",
    "lstm_candidates = [\n",
    "    '../models/trained_models/lstm_model.h5',\n",
    "    '../models/trained_models/lstm_model.keras',\n",
    "    'lstm_model.keras',\n",
    "    '../notebooks/lstm_model.keras',\n",
    "]\n",
    "\n",
    "lstm_path = next((p for p in lstm_candidates if os.path.exists(p)), None)\n",
    "\n",
    "if lstm_path:\n",
    "    try:\n",
    "        # Try different Keras import methods\n",
    "        try:\n",
    "            from tensorflow.keras.models import load_model\n",
    "        except ImportError:\n",
    "            try:\n",
    "                from keras.models import load_model\n",
    "            except ImportError:\n",
    "                print(\"Keras/TensorFlow not available for LSTM loading\")\n",
    "                load_model = None\n",
    "        \n",
    "        if load_model:\n",
    "            # Load without compiling to avoid metric issues\n",
    "            models['lstm'] = load_model(lstm_path, compile=False)\n",
    "            print(f\"Loaded LSTM model from {lstm_path}\")\n",
    "    except Exception as e:\n",
    "        print(f'Failed to load LSTM model: {e}')\n",
    "else:\n",
    "    print(\"No LSTM model file found\")\n",
    "\n",
    "print(f\"\\nLoaded models: {sorted(list(models.keys()))}\")\n",
    "\n",
    "# Create dummy models if none found (for testing)\n",
    "if not models:\n",
    "    print(\"\\nNo models found. Creating dummy models for testing...\")\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Train simple models\n",
    "    dummy_rf = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "    dummy_lr = LinearRegression()\n",
    "    \n",
    "    dummy_rf.fit(X_train, y_train)\n",
    "    dummy_lr.fit(X_train, y_train)\n",
    "    \n",
    "    models = {\n",
    "        'random_forest': dummy_rf,\n",
    "        'linear_regression': dummy_lr\n",
    "    }\n",
    "    print(f\"Created dummy models: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models (NRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating xgboost...\n",
      "xgboost NRMSE: 0.04026\n",
      "Prediction range: [0.00, 0.06]\n",
      "\n",
      "Evaluating lightgbm...\n",
      "lightgbm NRMSE: 0.02315\n",
      "Prediction range: [-0.00, 0.06]\n",
      "\n",
      "Evaluating lstm...\n",
      "lstm NRMSE: 11.05651\n",
      "Prediction range: [-1.12, 0.28]\n",
      "\n",
      "=== FINAL RESULTS ===\n",
      "lightgbm: 0.02315\n",
      "xgboost: 0.04026\n",
      "lstm: 11.05651\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        print(f\"\\nEvaluating {name}...\")\n",
    "        \n",
    "        if name == 'lstm':\n",
    "            # LSTM expects 3D input: (samples, timesteps, features)\n",
    "            X_test_reshaped = np.expand_dims(X_test, axis=1)\n",
    "            y_pred = model.predict(X_test_reshaped, verbose=0).ravel()\n",
    "        elif name == 'lightgbm':\n",
    "            # Convert to DataFrame to avoid feature name warnings\n",
    "            X_test_df = pd.DataFrame(X_test, columns=[f'feature_{i}' for i in range(X_test.shape[1])])\n",
    "            y_pred = model.predict(X_test_df)\n",
    "        else:\n",
    "            # Standard sklearn-like models\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate NRMSE\n",
    "        nrmse_score = nrmse(y_test, y_pred)\n",
    "        results[name] = float(nrmse_score)\n",
    "        predictions[name] = y_pred\n",
    "        \n",
    "        print(f\"{name} NRMSE: {nrmse_score:.5f}\")\n",
    "        print(f\"Prediction range: [{y_pred.min():.2f}, {y_pred.max():.2f}]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n=== FINAL RESULTS ===\")\n",
    "for name, score in sorted(results.items(), key=lambda x: x[1]):\n",
    "    print(f\"{name}: {score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating plots for xgboost: plot_predictions() got an unexpected keyword argument 'title'\n",
      "Error generating plots for lightgbm: plot_predictions() got an unexpected keyword argument 'title'\n",
      "Error generating plots for lstm: plot_predictions() got an unexpected keyword argument 'title'\n",
      "\n",
      "Plots saved to ../reports/figures/\n"
     ]
    }
   ],
   "source": [
    "# Ensure reports/figures directory exists\n",
    "os.makedirs('../reports/figures', exist_ok=True)\n",
    "\n",
    "# Generate plots for each model\n",
    "for name in results.keys():\n",
    "    try:\n",
    "        y_pred = predictions[name]\n",
    "        \n",
    "        # Generate and save plots\n",
    "        plot_predictions(\n",
    "            y_test, y_pred, \n",
    "            title=f'{name.title()} Predictions vs Actual',\n",
    "            save_path=f'../reports/figures/{name}_predictions.png'\n",
    "        )\n",
    "        \n",
    "        plot_residuals(\n",
    "            y_test, y_pred,\n",
    "            title=f'{name.title()} Residuals',\n",
    "            save_path=f'../reports/figures/{name}_residuals.png'\n",
    "        )\n",
    "        \n",
    "        print(f\"Generated plots for {name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating plots for {name}: {e}\")\n",
    "\n",
    "print(f\"\\nPlots saved to ../reports/figures/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Summary to reports/model_performance.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance summary saved to: ../reports/model_performance.md\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE SUMMARY\n",
      "==================================================\n",
      "1. Lightgbm: 0.02315\n",
      "2. Xgboost: 0.04026\n",
      "3. Lstm: 11.05651\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Ensure reports directory exists\n",
    "os.makedirs('../reports', exist_ok=True)\n",
    "\n",
    "summary_path = '../reports/model_performance.md'\n",
    "\n",
    "# Create markdown content\n",
    "lines = [\n",
    "    '# Model Performance Evaluation',\n",
    "    '',\n",
    "    f'Evaluation performed on {len(y_test)} test samples.',\n",
    "    f'Metric: Normalized Root Mean Square Error (NRMSE)',\n",
    "    '',\n",
    "    '## Results',\n",
    "    '',\n",
    "    '| Model | NRMSE | Rank |',\n",
    "    '|-------|-------|------|'\n",
    "]\n",
    "\n",
    "# Sort results by NRMSE (lower is better)\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "\n",
    "for rank, (model_name, nrmse_score) in enumerate(sorted_results, 1):\n",
    "    lines.append(f'| {model_name.title()} | {nrmse_score:.5f} | {rank} |')\n",
    "\n",
    "lines.extend([\n",
    "    '',\n",
    "    '## Notes',\n",
    "    '',\n",
    "    '- Lower NRMSE values indicate better performance',\n",
    "    '- Test set size: ' + str(len(y_test)) + ' samples',\n",
    "    '- Train set size: ' + str(len(y_train)) + ' samples',\n",
    "    f'- Target variable range: [{y_test.min():.2f}, {y_test.max():.2f}]',\n",
    "    '',\n",
    "    '## Visualization',\n",
    "    '',\n",
    "    'Generated plots available in `reports/figures/`:',\n",
    "])\n",
    "\n",
    "for model_name in results.keys():\n",
    "    lines.extend([\n",
    "        f'- `{model_name}_predictions.png` - Predictions vs Actual',\n",
    "        f'- `{model_name}_residuals.png` - Residual Analysis'\n",
    "    ])\n",
    "\n",
    "# Write to file\n",
    "content = '\\n'.join(lines)\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(f\"Model performance summary saved to: {summary_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for rank, (model_name, nrmse_score) in enumerate(sorted_results, 1):\n",
    "    print(f\"{rank}. {model_name.title()}: {nrmse_score:.5f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing time series cross-validation...\n",
      "Created 3 time series folds\n",
      "Fold 1: Train=2190, Validation=2190\n",
      "Fold 2: Train=4380, Validation=2190\n",
      "Fold 3: Train=6570, Validation=2190\n",
      "\n",
      "Evaluating lightgbm with time series CV...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7881\n",
      "[LightGBM] [Info] Number of data points in the train set: 2190, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.020013\n",
      "  Fold 1 NRMSE: 0.12328\n",
      "  Fold 2 failed: Input y contains NaN.\n",
      "  Fold 3 failed: Input y contains NaN.\n",
      "\n",
      "Time Series CV Results for lightgbm:\n",
      "Mean NRMSE: 0.12328 ± 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate time series validation\n",
    "try:\n",
    "    print(\"Performing time series cross-validation...\")\n",
    "    \n",
    "    # Create time series splits\n",
    "    ts_folds = list(time_series_split(X.values, y, n_splits=3))\n",
    "    print(f\"Created {len(ts_folds)} time series folds\")\n",
    "    \n",
    "    # Show fold sizes\n",
    "    for i, (train_idx, val_idx) in enumerate(ts_folds):\n",
    "        print(f\"Fold {i+1}: Train={len(train_idx)}, Validation={len(val_idx)}\")\n",
    "    \n",
    "    # Example: evaluate best model using time series CV\n",
    "    if results:\n",
    "        best_model_name = min(results, key=results.get)\n",
    "        best_model = models[best_model_name]\n",
    "        \n",
    "        print(f\"\\nEvaluating {best_model_name} with time series CV...\")\n",
    "        \n",
    "        cv_scores = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(ts_folds):\n",
    "            X_fold_train, X_fold_val = X.values[train_idx], X.values[val_idx]\n",
    "            y_fold_train, y_fold_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # Clone and retrain model for this fold\n",
    "            try:\n",
    "                from sklearn.base import clone\n",
    "                fold_model = clone(best_model)\n",
    "                fold_model.fit(X_fold_train, y_fold_train)\n",
    "                \n",
    "                if best_model_name == 'lightgbm':\n",
    "                    X_fold_val_df = pd.DataFrame(X_fold_val, columns=[f'feature_{i}' for i in range(X_fold_val.shape[1])])\n",
    "                    y_fold_pred = fold_model.predict(X_fold_val_df)\n",
    "                else:\n",
    "                    y_fold_pred = fold_model.predict(X_fold_val)\n",
    "                \n",
    "                fold_nrmse = nrmse(y_fold_val, y_fold_pred)\n",
    "                cv_scores.append(fold_nrmse)\n",
    "                print(f\"  Fold {fold+1} NRMSE: {fold_nrmse:.5f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Fold {fold+1} failed: {e}\")\n",
    "        \n",
    "        if cv_scores:\n",
    "            mean_cv_score = np.mean(cv_scores)\n",
    "            std_cv_score = np.std(cv_scores)\n",
    "            print(f\"\\nTime Series CV Results for {best_model_name}:\")\n",
    "            print(f\"Mean NRMSE: {mean_cv_score:.5f} ± {std_cv_score:.5f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in time series validation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
